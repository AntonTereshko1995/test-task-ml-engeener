model:
  baseline:
    model_name: "bert-base-uncased"  # Pretrained BERT model
    max_length: 128
    batch_size: 16
    learning_rate: 2e-5
    num_train_epochs: 5
    weight_decay: 0.01
    dropout: 0.1
    optimizer: "adamw"
    eval_strategy: "epoch"
    save_strategy: "epoch"
    logging_steps: 50
    warmup_ratio: 0.1
    fp16: True  # Mixed precision training
  tune:
    cv: 5
    metric: "f1"
    search:
      learning_rate: [1e-5, 2e-5, 3e-5]
      batch_size: [8, 16]
      num_train_epochs: [3, 5, 7]
      weight_decay: [0.01, 0.1]
      dropout: [0.1, 0.2]
data:
  label: "Title"
  text_column: "Title"
  features: ["Column 1", "Column 2", "Column 3", "Column 4"]
  test_size: 0.3